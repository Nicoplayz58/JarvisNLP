# ğŸ§  Procesamiento del Lenguaje Natural â€” Jarvis Calling Hiring Contest

Este proyecto aborda un desafÃ­o de **clasificaciÃ³n de texto** planteado en el *Jarvis Calling Hiring Contest*, mediante la implementaciÃ³n y comparaciÃ³n de diferentes enfoques de **Machine Learning** y **Deep Learning** aplicados al **Procesamiento del Lenguaje Natural (NLP)**.

El objetivo principal es evaluar la capacidad de distintos modelos para capturar patrones lingÃ¼Ã­sticos y semÃ¡nticos en los datos, analizando su rendimiento en tÃ©rminos de precisiÃ³n, generalizaciÃ³n y eficiencia computacional.

---

## ğŸ§© MetodologÃ­a general

El desarrollo del proyecto se estructura en **cuatro etapas principales**:

### 1ï¸âƒ£ AnÃ¡lisis Exploratorio de Datos (EDA)
Basado en las recomendaciones del artÃ­culo *Complete Guide to EDA on Text Data*, se realiza un estudio preliminar del conjunto de datos que incluye:

- DistribuciÃ³n de clases y longitud de textos.  
- Frecuencia de palabras, *word clouds* y anÃ¡lisis de *n-gramas*.  
- ExploraciÃ³n de patrones lÃ©xicos y semÃ¡nticos relevantes.

### 2ï¸âƒ£ Preprocesamiento del texto
Se implementan tÃ©cnicas de limpieza y normalizaciÃ³n del corpus:

- EliminaciÃ³n de sÃ­mbolos, *stopwords* y caracteres no alfabÃ©ticos.  
- TokenizaciÃ³n y secuenciaciÃ³n adaptada a cada arquitectura.  
- Uso de *embeddings* preentrenados (Word2Vec, FastText).

### 3ï¸âƒ£ ImplementaciÃ³n de modelos
Se entrenan y comparan **cinco modelos representativos** de distintas familias de arquitecturas:

| NÂº | Modelo | DescripciÃ³n breve |
|----|---------|-------------------|
| 1 | **DistilBERT / RoBERTa-base (fine-tuned)** | Ajuste fino de un Transformer preentrenado. |
| 2 | **Word2Vec + BiLSTM** | Modelo secuencial con memoria a largo plazo. |
| 3 | **CNN-1D para texto** | ExtracciÃ³n de caracterÃ­sticas locales mediante convoluciones. |
| 4 | **TF-IDF + XGBoost (GPU)** | RepresentaciÃ³n clÃ¡sica combinada con aprendizaje de gradiente. |
| 5 | **FastText** | Entrenamiento local con embeddings de subpalabras. |

> âš™ï¸ Todos los modelos fueron entrenados con conjuntos de entrenamiento, validaciÃ³n y prueba predefinidos.  
> No se aplicÃ³ *GridSearch* sistemÃ¡tico por limitaciones de tiempo y recursos.

### 4ï¸âƒ£ EvaluaciÃ³n y comparaciÃ³n
Se evalÃºan los modelos con mÃ©tricas estÃ¡ndar de clasificaciÃ³n:

- **Accuracy**, **Precision**, **Recall**, **F1-Score (Macro)**  
- **Matriz de confusiÃ³n**  
- **ROC-AUC (One-vs-Rest)**  
- **Tiempos de entrenamiento y uso de GPU**

---

## ğŸ“š Estructura del Jupyter Book

El proyecto estÃ¡ documentado y publicado como un **Jupyter Book** con las siguientes secciones:

- `EDA/` â€” AnÃ¡lisis exploratorio del conjunto de datos  
- `DistilBERT/` â€” Ajuste fino del modelo Transformer  
- `TF Models/` â€” Implementaciones de BiLSTM, CNN-1D y FastText  
- `Resultados/` â€” EvaluaciÃ³n comparativa y anÃ¡lisis crÃ­tico  

ğŸ‘‰ **VersiÃ³n en lÃ­nea:**  
ğŸ”— [https://nicoplayz58.github.io/JarvisNLP/](https://nicoplayz58.github.io/JarvisNLP/)

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Python 3.12**  
- **Jupyter Book**  
- **scikit-learn**, **TensorFlow**, **PyTorch**  
- **Transformers (Hugging Face)**  
- **XGBoost (GPU)**  
- **FastText**, **Gensim**, **NLTK**, **spaCy**

---

## ğŸ“¦ Estructura del repositorio

